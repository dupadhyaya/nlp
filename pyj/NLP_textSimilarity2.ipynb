{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f24f3c",
   "metadata": {},
   "source": [
    "# Finding Text Similarity\n",
    "-  https://www.scaler.com/topics/nlp/text-similarity-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a748a3e",
   "metadata": {},
   "source": [
    "## which pairs are similar\n",
    "-  Global warming is here. Ocean temperature is rising\n",
    "-  I'm reading a book. The is book is about nlp.\n",
    "-  Text similarity in nlp is easy. I like data science.\n",
    "-  This place is great. This is great news.\n",
    "-  It might not rain today. It might not work today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e33c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 2 pair, similar\n",
    "# next 3 not similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123e1f4",
   "metadata": {},
   "source": [
    "## The Similarity Problem\n",
    "-  We are trying to find text similarity using NLP or just some similarity between texts. To find this, we must first define two aspects.\n",
    "    -  The method that will be used to calculate the similarities.\n",
    "    -  The algorithm that will be used for the transformation of our texts to embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc7379",
   "metadata": {},
   "source": [
    "## Pre-processing for Text Similarity\n",
    "-  Now, before we get started with the two aspects discussed above, we must first pre-process our text to find out text similarity using NLP. The pipeline that we are going to use for pre-processing is - Normalization, Tokenization, Removal of Stop Words, Stemming & Lemmatization.\n",
    "-  For this pre-processing pipeline, we will make use of one of the most popular libraries in python - NLTK. Here's the code for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b526523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import regex as re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51877979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function pre-processes our text\n",
    "def preProcessText(text):\n",
    "    processed = []\n",
    "    for doc in range(len(text)):\n",
    "        doc = re.sub(r\"\\\\n\", \"\", doc)\n",
    "        doc = re.sub(r\"\\W\", \" \", doc) #remove non words char\n",
    "        doc = re.sub(r\"\\d\",\" \", doc) #remove digits char\n",
    "        doc = re.sub(r'\\s+[a-z]\\s+', \"\", doc) # remove a single char\n",
    "        doc = re.sub(r'^[a-z]\\s+', \"\", doc) #remove a single character at the start of a document\n",
    "        doc = re.sub(r'\\s+', \" \", doc)  #replace an extra space with a single space\n",
    "        doc = re.sub(r'^\\s', \"\", doc) # remove space at the start of a doc\n",
    "        doc = re.sub(r'\\s$', \"\", doc) # remove space at the end of a document\n",
    "        processed.append(doc.lower())\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a55d239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split('W+',text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef88664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words\n",
    "stops = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fef55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    text = [word for word in text if word not in stops]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c052a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of text\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed12b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemm_text = [wnl.lemmatize(word) for word in text]\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224d7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    text = list(map(preProcessText, text))\n",
    "    text = list(map(tokenize, text))\n",
    "    text = list(map(removeStopWords, text))\n",
    "    text = list(map(lemmatize, text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27461008",
   "metadata": {},
   "source": [
    "#### Once all this preprocessing is done, the next task in hand is to decide the algorithm that we would use to convert our processed text into embeddings. A few of the algorithms that can be used are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf1b87",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "-  TF-IDF is one of the methods that can be used to convert text to embeddings. It is short for Term Frequency - Inverse Document Frequency. Before moving on to the concept of TF-IDF, let's discuss a very basic document-term matrix.\n",
    "-  To build a document-term matrix you only need a matrix where each row is a phrase/sentence or in NLP terms - a document and every column is a word. The values filled in the matrix are the number of times a word (column) appeared in that document (row).\n",
    "-  Hence, the document term matrix for our 5th pair of sentences would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020663c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37c3f43e",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "-  This technique makes use of neural networks to represent our documents as embeddings. The Word2Vec model usually also captures the contextual meaning of words very well. The embeddings are represented as multidimensional arrays. To generate the word embeddings using Word2Vec, there are two unsupervised algorithms - Skip Gram or Continuous Bag of Words (CBOW). Both of these algorithms are architectures that make use of neural networks to learn the underlying word representations.\n",
    "-  https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html\n",
    "-  https://www.analyticsvidhya.com/blog/2023/07/step-by-step-guide-to-word2vec-with-gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ebfac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf76a0e6",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words (CBOW)\n",
    "-  In this model, the distributed representations of context, or in simpler terms, the surrounding words are combined to predict the word that lies in the middle.\n",
    "-  For example, if we had a sentence - \"I like to drink coffee the whole day\", here's what the inputs and outputs of the cbow model would look like -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c91711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2483976c",
   "metadata": {},
   "source": [
    "### Skip-Gram\n",
    "-  The Skip-Gram model is the complete opposite of the continuous bag of words (CBOW) model. Here, instead of using the surrounding words to predict the middle word, we pass as input a target word to predict the neighboring words.\n",
    "-  For the same sentence as above, the inputs and outputs of the skip-gram model would look like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56df109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bad5ae8b",
   "metadata": {},
   "source": [
    "### Methods to Find Text Similarity in NLP\n",
    "-  after discussing some algorithms that can be used to convert the text into embeddings, we can move on to the second aspect -- the statistical method that will be used to compute similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d2c6b",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "-  Cosine similarity is most definitely the most widely used method to compare vectors. In simple terms, cosine similarity is just the dot product between two vectors or the cosine angle between two vectors.\n",
    "-  To find the dot product between two vectors A and B, we use the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9b9c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_cosine_similarity(vec_1, vec_2):  \n",
    "    return cosine_similarity(vec_1.reshape(1, -1), vec_2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066564a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.452270576122071\n"
     ]
    }
   ],
   "source": [
    "# finding cosine similarity between two vectors\n",
    "vec1 = np.array([[12, 41, 60, 11, 21]])\n",
    "vec2 = np.array([[40, 11, 4, 11, 14]])\n",
    "print(get_cosine_similarity(vec1, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91f37c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9983311419668022\n"
     ]
    }
   ],
   "source": [
    "# creating a vector similar to vec1\n",
    "vec3 = np.array([[12, 45, 60, 11, 25]])\n",
    "print(get_cosine_similarity(vec1, vec3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33636d23",
   "metadata": {},
   "source": [
    "- The output - 1 above signifies that the two vectors are not very similar, and not very different either. For the second output, since vec3 has almost all same values as vec1, we can see that the cosine similarity is high - 0.9983, signifying that the two vectors are extremely similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3036149",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "-  Moving on to another similarity metric - Euclidean Distance. This is a simple metric that measures the distance between two points by making use of the Pythagorean theorem. To calculate the Euclidean distance between two vectors, simply apply the following formula to the calculated document vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c715b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def euclideanDistance(vec1: [int], vec2: [int]) -> float:\n",
    "    return sqrt(sum(pow(x - y, 2) for x, y in zip(vec1, vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3756cf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0710678118654755\n"
     ]
    }
   ],
   "source": [
    "vec_1 = [1, 3, 2]\n",
    "vec_2 = [5, 0, -3]\n",
    "print(euclideanDistance(vec_1, vec_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b28fd",
   "metadata": {},
   "source": [
    "### Word Moverâ€™s Distance\n",
    "-  In both the similarity metrics discussed above, we did not take into consideration semantics. The Word Mover's distance tries to measure the semantic distance between two documents. The Word Mover's Distance leverages the results obtained from other advanced word embedding techniques such as Glove and word2vec to generate its embeddings that can be scaled to very large data sets. With these embedding techniques, we can preserve semantic relationships.\n",
    "-  The word mover's distance between two text documents is calculated by the minimum cumulative distance that words from text document A need to travel to match exactly the point cloud of text document B.\n",
    "-  https://radimrehurek.com/gensim/models/word2vec.html\n",
    "-  https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2e4e5",
   "metadata": {},
   "source": [
    "A couple of intriguing properties of word mover's distance are:\n",
    "\n",
    "It does not have any hyper-parameters and is pretty straightforward to understand and use!\n",
    "It is quite comprehensible since the distance between documents may be taken down and explained as the sparse distance between a couple of individual words.\n",
    "The knowledge encoded in word2vec and glove is naturally incorporated which leads to high accuracy.\n",
    "Let's now look at an example with code implementation of the word mover's distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfb5e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_a = \"Modi had a chat with Bear Grylls in Jim Corbett\"\n",
    "sent_b = \"The Prime Minister met the TV host in a national park\"\n",
    "\n",
    "sent_a = sent_a.lower().split()\n",
    "sent_b = sent_b.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baefd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "sent_a = [word for word in sent_a if word not in stop]\n",
    "sent_b = [word for word in sent_b if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f089fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "#model = Word2Vec()\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff3fb49d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'wmdistance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dist1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwmdistance\u001b[49m(sent_a, sent_b)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(dist1)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# let's write a sentence that is not so similar to sent_a\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'wmdistance'"
     ]
    }
   ],
   "source": [
    "dist1 = model.wmdistance(sent_a, sent_b)\n",
    "print(dist1)\n",
    "\n",
    "# let's write a sentence that is not so similar to sent_a\n",
    "sent_c = \"Leos are born in August\"\n",
    "sent_c = sent_c.lower().split()\n",
    "sent_c = [word for word in sent_c if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c11f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2 = model.wmdistance(sent_a, sent_c)\n",
    "print(dist2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
